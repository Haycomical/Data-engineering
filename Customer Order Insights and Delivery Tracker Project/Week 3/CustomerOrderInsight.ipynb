{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/mydrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IpFRDM-NsdKG",
        "outputId": "b3703217-1179-4672-b737-1c66fdf63dab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/mydrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/mydrive/MyDrive/cleaned_orders.csv'\n",
        "df = pd.read_csv(path)"
      ],
      "metadata": {
        "id": "j2OsmSYSsvpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4hqnVT9rwHF",
        "outputId": "44ed9f1b-1947-44b2-e49e-b22448d39576"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----------+\n",
            "|region|delay_count|\n",
            "+------+-----------+\n",
            "| South|          2|\n",
            "|  East|          3|\n",
            "|  West|          3|\n",
            "| North|          2|\n",
            "+------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, count\n",
        "\n",
        "spark = SparkSession.builder.appName(\"OrderAnalysis\").getOrCreate()\n",
        "\n",
        "path = '/content/mydrive/MyDrive/cleaned_orders.csv'\n",
        "\n",
        "df = spark.read.csv(path, header=True, inferSchema=True)\n",
        "\n",
        "delays_by_region = df.filter(col(\"delayed\") == 1).groupBy(\"region\").agg(count(\"*\").alias(\"delay_count\"))\n",
        "\n",
        "\n",
        "delays_by_region.show()\n",
        "\n",
        "\n",
        "delays_by_region.write.csv(\"delays_by_region.csv\", header=True, mode=\"overwrite\")\n"
      ]
    }
  ]
}